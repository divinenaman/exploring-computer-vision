{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91f4f28-8fd2-47a9-9c5d-48ce63ad295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 22:14:17.494956: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 22:14:17.495056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 22:14:17.519570: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 22:14:17.579026: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 22:14:18.702609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/naman/dev/exploring-computer-vision/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5047500d-b4c9-4193-8ee0-5f57e846547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.cmap'] = 'Greys'\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.get_visible_devices())\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da7b446d-0b8c-40c4-8e64-eb668bd1c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test) = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)\n",
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b82e751-4efe-48d0-9de9-4637dfb4928f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
       " <_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img)\n",
    "ds_train = ds_train.batch(100)\n",
    "\n",
    "ds_test_norm = ds_test.map(normalize_img)\n",
    "ds_test = ds_test_norm.batch(100)\n",
    "\n",
    "(ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a03295e8-dd4f-410b-8756-bdfdb2ddb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wx + b\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "# loss fn\n",
    "cross_en = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "model.compile(loss=cross_en, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be790341-b459-48db-a393-3b6b07009746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      " softmax_2 (Softmax)         (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7850 (30.66 KB)\n",
      "Trainable params: 7850 (30.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'name': 'sequential_2', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 28, 28), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'flatten_2_input'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Flatten', 'config': {'name': 'flatten_2', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 28, 28), 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': (None, 28, 28)}}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 784)}}, {'module': 'keras.layers', 'class_name': 'Softmax', 'config': {'name': 'softmax_2', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'registered_name': None, 'build_config': {'input_shape': (None, 10)}}]}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"training-simple-mnist/1000.ckpt\"\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    model.fit(ds_train, epochs=1000)\n",
    "    model.save_weights(checkpoint)\n",
    "\n",
    "print(model.summary())\n",
    "print(model.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1890e69b-966a-4f3b-92fe-d61a5f157123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 927us/step\n",
      "[[0.00178988 0.12952857 0.05930356 0.01328077 0.04860185 0.62771136\n",
      "  0.00342846 0.09864821 0.00262953 0.01507791]\n",
      " [0.00220434 0.11356433 0.05972067 0.01206429 0.04779417 0.6457465\n",
      "  0.00398579 0.0982242  0.00295765 0.01373812]\n",
      " [0.0017458  0.11850025 0.05440576 0.01304988 0.05354543 0.6133646\n",
      "  0.00352935 0.12242266 0.002714   0.01672228]\n",
      " [0.00177847 0.1470602  0.05517699 0.01248505 0.04655268 0.6056687\n",
      "  0.00351205 0.10938215 0.00271061 0.01567311]\n",
      " [0.00176678 0.12640393 0.05482807 0.01299749 0.04863904 0.6055508\n",
      "  0.00331347 0.12744038 0.00258864 0.01647137]\n",
      " [0.0017769  0.13068055 0.05700647 0.01255093 0.05005436 0.62328464\n",
      "  0.00400692 0.1030016  0.00268008 0.01495748]\n",
      " [0.00205562 0.11149572 0.05621679 0.01227714 0.04647167 0.6389664\n",
      "  0.00329309 0.11094745 0.00275246 0.01552371]\n",
      " [0.00177824 0.14011261 0.05573915 0.01292163 0.04884734 0.6209904\n",
      "  0.0037301  0.09837438 0.00269171 0.01481437]\n",
      " [0.00177177 0.13289863 0.05724012 0.01362977 0.04502409 0.6352941\n",
      "  0.00362202 0.09389435 0.00272742 0.01389768]\n",
      " [0.00168673 0.1521705  0.05622323 0.01280313 0.04697694 0.598526\n",
      "  0.00357548 0.11008636 0.00266257 0.01528908]\n",
      " [0.00169549 0.13129434 0.0541478  0.0124177  0.04712581 0.62866515\n",
      "  0.00361209 0.1034323  0.00273738 0.01487201]\n",
      " [0.00207317 0.11976754 0.05765116 0.01319439 0.04888241 0.6361196\n",
      "  0.00381174 0.10020036 0.00279128 0.01550826]\n",
      " [0.00171379 0.12344501 0.05403482 0.01331679 0.04837173 0.61077964\n",
      "  0.00329087 0.12596321 0.00257138 0.01651281]\n",
      " [0.00168942 0.13400045 0.05263624 0.01299941 0.05031595 0.6058005\n",
      "  0.00302067 0.12020039 0.00271777 0.0166192 ]\n",
      " [0.00179959 0.13624077 0.05623676 0.01277633 0.04854349 0.61558956\n",
      "  0.00303889 0.10694848 0.00278475 0.01604138]\n",
      " [0.00184062 0.11230047 0.05239653 0.01153258 0.05460922 0.6268081\n",
      "  0.00357559 0.11785638 0.00279249 0.01628809]\n",
      " [0.00176349 0.12980089 0.05333963 0.0117708  0.04832362 0.6262242\n",
      "  0.00336554 0.10779406 0.00261696 0.01500076]\n",
      " [0.0017721  0.13662401 0.05491359 0.01331342 0.04760139 0.6124604\n",
      "  0.00357253 0.11116561 0.00266571 0.01591127]\n",
      " [0.00163214 0.14491346 0.05277026 0.01283056 0.05043224 0.6020164\n",
      "  0.00329098 0.11361695 0.00263354 0.01586348]\n",
      " [0.00191139 0.12823184 0.0547525  0.01278258 0.04874115 0.6248572\n",
      "  0.00304238 0.10746963 0.00269052 0.01552084]\n",
      " [0.00172795 0.14168084 0.05593367 0.01251802 0.0490325  0.59582675\n",
      "  0.00374566 0.12119775 0.00254444 0.01579244]\n",
      " [0.0016381  0.14291717 0.05630523 0.01277727 0.049575   0.6008945\n",
      "  0.00374141 0.1139891  0.00261229 0.0155499 ]\n",
      " [0.00176116 0.13312863 0.05628502 0.01363246 0.04665451 0.6201174\n",
      "  0.00351942 0.10871191 0.00256708 0.01362233]\n",
      " [0.0019715  0.13196947 0.05808833 0.01301653 0.0489498  0.6238511\n",
      "  0.00369865 0.10117765 0.00272574 0.01455116]\n",
      " [0.00181659 0.15131645 0.06052111 0.01371548 0.0447327  0.6153509\n",
      "  0.00379883 0.09196656 0.00264397 0.01413742]\n",
      " [0.00175488 0.12942405 0.05700093 0.01411092 0.04512278 0.6304962\n",
      "  0.00316047 0.10163775 0.00271041 0.01458166]\n",
      " [0.00172652 0.14611672 0.05529406 0.0128352  0.0484664  0.6106863\n",
      "  0.00371119 0.10390235 0.00263935 0.0146219 ]\n",
      " [0.00174145 0.13253185 0.05674875 0.01252517 0.04868999 0.6175569\n",
      "  0.00396604 0.10840356 0.00263732 0.01519896]\n",
      " [0.00183011 0.13753566 0.05718974 0.01270207 0.05085808 0.59041196\n",
      "  0.0031975  0.12704857 0.00271362 0.01651263]\n",
      " [0.00177679 0.13184862 0.05594818 0.01249238 0.05213464 0.6021361\n",
      "  0.00356119 0.12085094 0.00267084 0.01658037]\n",
      " [0.00181025 0.12541725 0.05236273 0.01279015 0.05050914 0.61898994\n",
      "  0.00330844 0.11588243 0.00267287 0.01625679]\n",
      " [0.00171678 0.13401224 0.05537887 0.01356604 0.0477653  0.61328506\n",
      "  0.00277859 0.11314436 0.00265963 0.01569312]\n",
      " [0.00170608 0.1453922  0.05587248 0.01285724 0.04795542 0.6143327\n",
      "  0.0035613  0.10075201 0.00278378 0.01478681]\n",
      " [0.00161455 0.1424689  0.05381726 0.01252734 0.04923511 0.6013875\n",
      "  0.00344981 0.11687091 0.00261198 0.01601673]\n",
      " [0.00172669 0.12860347 0.05469026 0.0128364  0.04894762 0.632223\n",
      "  0.00369353 0.09979954 0.00266781 0.01481164]\n",
      " [0.00178573 0.11910906 0.05287281 0.01162559 0.05365459 0.6220186\n",
      "  0.00354827 0.11645618 0.00283921 0.01608997]\n",
      " [0.0017712  0.12366912 0.05928498 0.01371767 0.04865455 0.62865406\n",
      "  0.00360134 0.10255196 0.00270915 0.01538592]\n",
      " [0.00168585 0.13428992 0.05234819 0.01142152 0.04783349 0.62407357\n",
      "  0.00333318 0.10772217 0.00265458 0.01463759]\n",
      " [0.00173011 0.141215   0.0551431  0.01267883 0.04839608 0.6082076\n",
      "  0.00347054 0.11138971 0.00257908 0.01518998]\n",
      " [0.00173198 0.14321166 0.05569405 0.01283013 0.04823591 0.6180749\n",
      "  0.00342975 0.09952062 0.00273889 0.01453216]\n",
      " [0.00175606 0.13393298 0.05146373 0.01277952 0.04704406 0.62830204\n",
      "  0.0031485  0.10397869 0.00265303 0.01494141]\n",
      " [0.00169104 0.14562821 0.0578654  0.01305405 0.04616903 0.6108624\n",
      "  0.00329687 0.10279141 0.00274923 0.01589232]\n",
      " [0.00168613 0.14358276 0.05488261 0.01270906 0.0511471  0.59597385\n",
      "  0.00347184 0.1175545  0.00264902 0.01634311]\n",
      " [0.00187117 0.13840446 0.06377756 0.0135258  0.04904323 0.600295\n",
      "  0.00415596 0.11065052 0.00271289 0.01556345]\n",
      " [0.00178349 0.12568165 0.05391041 0.01245509 0.05241049 0.6109896\n",
      "  0.00334691 0.12020911 0.00269471 0.01651863]\n",
      " [0.00173662 0.15266877 0.06209137 0.01399748 0.04382355 0.61689514\n",
      "  0.00372473 0.08885398 0.00276679 0.01344167]\n",
      " [0.00167734 0.14204115 0.05580434 0.01288592 0.04931912 0.5977175\n",
      "  0.0034241  0.11834791 0.0026578  0.01612486]\n",
      " [0.00218814 0.10007992 0.05968744 0.01278528 0.04918499 0.6377951\n",
      "  0.00381116 0.11539711 0.00287316 0.01619783]\n",
      " [0.001733   0.13091545 0.05110577 0.01269957 0.04827645 0.62337714\n",
      "  0.00281743 0.11122491 0.00263985 0.01521043]\n",
      " [0.0016937  0.15464844 0.0565441  0.01291691 0.04769957 0.60232645\n",
      "  0.00357792 0.10329627 0.00267913 0.01461746]\n",
      " [0.00193607 0.12624115 0.05736189 0.01309229 0.04898668 0.62520057\n",
      "  0.00363113 0.10552225 0.00271257 0.01531545]\n",
      " [0.00171156 0.13399635 0.05314085 0.01319932 0.04907128 0.6023074\n",
      "  0.00330606 0.12432127 0.00259675 0.01634911]\n",
      " [0.00162137 0.13876599 0.05413174 0.01272591 0.05006204 0.60399824\n",
      "  0.00333736 0.11660758 0.00262792 0.01612185]\n",
      " [0.00181042 0.12219043 0.05576941 0.01260641 0.05146848 0.6140931\n",
      "  0.00345323 0.11927583 0.00269391 0.01663868]\n",
      " [0.00181824 0.12675218 0.05522771 0.0125801  0.05311621 0.6006602\n",
      "  0.00355826 0.12610352 0.00279553 0.01738809]\n",
      " [0.00169966 0.14126086 0.05809632 0.01278312 0.05002117 0.6043186\n",
      "  0.00405403 0.10960861 0.00265806 0.0154995 ]\n",
      " [0.0017135  0.13557188 0.05414163 0.0128597  0.04788247 0.6153846\n",
      "  0.00320317 0.11112919 0.00263328 0.01548063]\n",
      " [0.0016967  0.1369115  0.05544511 0.01282932 0.04901877 0.61381674\n",
      "  0.00341435 0.10843933 0.00272811 0.01570002]\n",
      " [0.00180877 0.11867483 0.05566508 0.01105761 0.04938568 0.6374421\n",
      "  0.0035443  0.10361134 0.00288926 0.01592103]\n",
      " [0.00178877 0.1342859  0.06009679 0.01291613 0.04823292 0.6171852\n",
      "  0.0039808  0.10514006 0.00269395 0.01367953]\n",
      " [0.00160716 0.12488566 0.05515362 0.01293356 0.05035099 0.6155096\n",
      "  0.00341589 0.11705431 0.00266714 0.01642205]\n",
      " [0.00191862 0.11162862 0.05688099 0.0120211  0.05212168 0.62156224\n",
      "  0.00372358 0.12077678 0.00274832 0.01661808]\n",
      " [0.00169354 0.12806173 0.05012275 0.01166563 0.04757464 0.6316212\n",
      "  0.00326674 0.10870358 0.00262537 0.01466485]\n",
      " [0.00160802 0.1383196  0.05497337 0.01295803 0.05195693 0.5989177\n",
      "  0.00347775 0.11884494 0.00262381 0.01631993]\n",
      " [0.00166504 0.14896801 0.05990207 0.01340358 0.049327   0.5968072\n",
      "  0.00353877 0.1086148  0.00255441 0.01521915]\n",
      " [0.00174295 0.13063009 0.0580143  0.01284847 0.04984486 0.61417586\n",
      "  0.00404259 0.11069196 0.00264326 0.01536561]\n",
      " [0.00186757 0.1322853  0.05773006 0.01282197 0.04982103 0.61195534\n",
      "  0.00311212 0.11068484 0.00294003 0.0167818 ]\n",
      " [0.00167202 0.15138605 0.05536095 0.01288929 0.04719949 0.59992343\n",
      "  0.00352479 0.10998575 0.0026321  0.0154261 ]\n",
      " [0.00201075 0.12169887 0.05665708 0.01282955 0.05091432 0.61327165\n",
      "  0.00362325 0.12046637 0.00253569 0.01599251]\n",
      " [0.00171655 0.13903503 0.05792717 0.01291009 0.0492789  0.6112872\n",
      "  0.0038808  0.10651575 0.00264978 0.01479886]\n",
      " [0.00167606 0.13483955 0.05306771 0.0128314  0.04870553 0.6108169\n",
      "  0.00345696 0.11590776 0.0025855  0.01611258]\n",
      " [0.00174453 0.13467285 0.05136355 0.01151017 0.04802344 0.61776114\n",
      "  0.00334375 0.11358549 0.0026106  0.01538447]\n",
      " [0.00177344 0.13683933 0.05404981 0.01283043 0.0485416  0.6275626\n",
      "  0.00366666 0.09757555 0.00266422 0.01449641]\n",
      " [0.0017691  0.13290234 0.05436637 0.01264834 0.0521236  0.6082348\n",
      "  0.00351955 0.11549549 0.0026627  0.0162777 ]\n",
      " [0.00161683 0.15379892 0.05497872 0.01278599 0.04734768 0.59462136\n",
      "  0.00352705 0.11336071 0.00262577 0.01533699]\n",
      " [0.00167844 0.14595965 0.05766175 0.01288165 0.04733191 0.60787356\n",
      "  0.00389755 0.10525998 0.00266375 0.0147918 ]\n",
      " [0.00176758 0.13388564 0.05709399 0.01317859 0.0496409  0.60305536\n",
      "  0.00321005 0.11947741 0.00267395 0.01601659]\n",
      " [0.00172749 0.12658675 0.05200616 0.01228332 0.0481002  0.63299793\n",
      "  0.00349769 0.10505395 0.00262624 0.01512029]\n",
      " [0.00175275 0.14092937 0.05948784 0.01235737 0.05222761 0.5980457\n",
      "  0.00375873 0.11298148 0.00258373 0.01587543]\n",
      " [0.00162781 0.1454927  0.05291753 0.01266574 0.05008435 0.59979314\n",
      "  0.00335028 0.11530387 0.002623   0.0161416 ]\n",
      " [0.00194544 0.12263553 0.05698952 0.01258076 0.0486371  0.6400947\n",
      "  0.00384965 0.09608561 0.00268576 0.01449589]\n",
      " [0.00184872 0.13093102 0.05947225 0.01244077 0.05109711 0.60482866\n",
      "  0.00411999 0.11655439 0.00269013 0.01601695]\n",
      " [0.00180139 0.12320425 0.05519769 0.01297527 0.05418999 0.61893195\n",
      "  0.00361846 0.11175409 0.00266483 0.01566208]\n",
      " [0.00166549 0.1430242  0.05409673 0.01243844 0.05163317 0.6025273\n",
      "  0.00344147 0.11246526 0.00264266 0.01606524]\n",
      " [0.00180294 0.13315898 0.0583626  0.0136667  0.04952117 0.59878397\n",
      "  0.00336442 0.12238204 0.00273798 0.01621922]\n",
      " [0.00167175 0.14193033 0.05483594 0.01270863 0.04906082 0.60592663\n",
      "  0.00349771 0.11215764 0.00265455 0.01555593]\n",
      " [0.00163384 0.13772333 0.05317438 0.01258407 0.04862245 0.6000935\n",
      "  0.00326715 0.12422077 0.00256107 0.01611942]\n",
      " [0.00169891 0.14226916 0.05574477 0.01300891 0.04686518 0.61032784\n",
      "  0.00340653 0.10832082 0.00274934 0.01560857]\n",
      " [0.00171026 0.1289284  0.05171922 0.01294959 0.04820263 0.62183446\n",
      "  0.00337413 0.11275718 0.00263535 0.01588879]\n",
      " [0.00198869 0.12870693 0.05583385 0.01278464 0.04840528 0.62082535\n",
      "  0.00365764 0.10934652 0.00267364 0.01577746]\n",
      " [0.00173865 0.12599157 0.05313981 0.01278271 0.05147806 0.6113651\n",
      "  0.00329325 0.12059031 0.00271584 0.01690471]\n",
      " [0.00201466 0.13169801 0.05988873 0.01235658 0.04807869 0.6198257\n",
      "  0.00375801 0.10531107 0.00286992 0.01419867]\n",
      " [0.00158452 0.15171546 0.05354569 0.01275273 0.04736894 0.60036427\n",
      "  0.00344998 0.11119945 0.00262401 0.01539502]\n",
      " [0.00163413 0.15386625 0.05417747 0.01286648 0.04745337 0.5970734\n",
      "  0.00350064 0.11130031 0.00261369 0.01551432]\n",
      " [0.00169072 0.12558632 0.05180089 0.01291618 0.04871709 0.615845\n",
      "  0.00341703 0.12086476 0.0025844  0.01657756]\n",
      " [0.00167103 0.14145985 0.05435414 0.01268823 0.05101852 0.6001983\n",
      "  0.00343266 0.11649893 0.00263694 0.01604133]\n",
      " [0.0016774  0.13879104 0.0527393  0.01238547 0.04786643 0.62019193\n",
      "  0.00341324 0.1058496  0.0026087  0.01447687]\n",
      " [0.00181612 0.12598498 0.05541002 0.01272048 0.05102244 0.6084889\n",
      "  0.00357393 0.12177777 0.00264863 0.01655674]\n",
      " [0.0016599  0.14668837 0.05473489 0.01275065 0.04808017 0.59827524\n",
      "  0.00340146 0.11617629 0.00258378 0.01564924]\n",
      " [0.00173147 0.13734421 0.05879182 0.01269268 0.04926587 0.60923636\n",
      "  0.00397807 0.10917166 0.00267584 0.01511203]]\n",
      "predicted:  [ 1 49 43 25 15  1 43  4  1 54] (0.6457465291023254)\n",
      "actual:  [2 0 4 8 7 6 0 6 3 1 8 0 7 9 8 4 5 3 4 0 6 6 3 0 2 3 6 6 7 4 9 3 8 7 5 4 2\n",
      " 5 5 8 5 2 9 2 4 2 7 0 5 1 0 7 9 9 9 6 5 8 8 6 9 9 5 4 2 6 8 1 0 6 9 5 5 4\n",
      " 1 6 7 5 2 9 0 6 4 4 2 8 7 8 3 0 9 0 1 1 9 4 5 9 7 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 23:56:28.467932: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "for d in ds_test.take(1):\n",
    "    img, label = d\n",
    "    img_, label_ = normalize_img(img, label)\n",
    "    # img_ = np.reshape(img_, (1, 28, 28))\n",
    "    out = model.predict(img_)\n",
    "    print(out)\n",
    "    print(\"predicted: \", out.argmax(axis=0), f\"({out.max()})\")\n",
    "    print(\"actual: \", label_.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5aa1c9b2-ff68-4755-949a-c1a3fe235089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 0s - 272ms/epoch - 3ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# overall test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_acc)\n",
      "File \u001b[0;32m~/dev/exploring-computer-vision/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/dev/exploring-computer-vision/venv/lib/python3.10/site-packages/keras/src/engine/training.py:4407\u001b[0m, in \u001b[0;36mflatten_metrics_in_order\u001b[0;34m(logs, metrics_names)\u001b[0m\n\u001b[1;32m   4405\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   4406\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m metrics_names:\n\u001b[0;32m-> 4407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m:\n\u001b[1;32m   4408\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(logs[name])\n\u001b[1;32m   4409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(logs\u001b[38;5;241m.\u001b[39mkeys()):\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "# overall test\n",
    "test_loss, test_acc = model.evaluate(ds_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
